# -*- coding: utf-8 -*-
"""
#--------------------ä½¿ç”¨ Keras ä¸­é¢„å®šä¹‰çš„ç»å…¸å·ç§¯ç¥ç»ç½‘ç»œç»“æ„-------------------------
#tf.keras.applications ä¸­æœ‰ä¸€äº›é¢„å®šä¹‰å¥½çš„ç»å…¸å·ç§¯ç¥ç»ç½‘ç»œç»“æ„ï¼Œå¦‚ VGG16 ã€ VGG19 ã€ ResNet ã€ MobileNet ç­‰ã€‚
#æˆ‘ä»¬å¯ä»¥ç›´æ¥è°ƒç”¨è¿™äº›ç»å…¸çš„å·ç§¯ç¥ç»ç½‘ç»œç»“æ„ï¼ˆç”šè‡³è½½å…¥é¢„è®­ç»ƒçš„å‚æ•°ï¼‰ï¼Œè€Œæ— éœ€æ‰‹åŠ¨å®šä¹‰ç½‘ç»œç»“æ„ã€‚

#https://tf.wiki/zh/basic/models.html

Created on Sat Dec 28 19:49:26 2019

@author: liujie
"""
import tensorflow as tf
import tensorflow_datasets as tfds

#-----------------------------å®šä¹‰ä¸€äº›æ¨¡å‹è¶…å‚æ•°------------------------------
# è¿™äº›å‚æ•°éƒ½ä¼šå½±å“æ¨¡å‹æ€§èƒ½
num_epochs = 1000
batch_size = 50 #æ‰¹æ¬¡å¤§å°å½±å“æ¨¡å‹æ€§èƒ½ï¼Œå°ä¸€ç‚¹ä¼šæ›´å¥½
learning_rate = 0.01

#-----------------------------æ•°æ®è·å–åŠé¢„å¤„ç†------------------------------
#æ•°æ®å‡†å¤‡æ–¹å¼å½±å“è®¡ç®—é€Ÿåº¦
dataset = tfds.load("tf_flowers", split=tfds.Split.TRAIN, as_supervised=True)
dataset = dataset.map(lambda img, label: (tf.image.resize(img, [224, 224]) / 255.0, label)).shuffle(1024).batch(32)

#-----------------------------æ¨¡å‹çš„æ„å»º------------------------------
#ä½¿ç”¨ Keras ä¸­é¢„å®šä¹‰çš„ç»å…¸å·ç§¯ç¥ç»ç½‘ç»œç»“æ„MobileNetV2
# æ¨¡å‹çš„è®¾è®¡å½±å“æ¨¡å‹çš„æ€§èƒ½
model = tf.keras.applications.MobileNetV2(weights=None, classes=5)

# ä¼˜åŒ–å™¨optimizerçš„ä½œç”¨ï¼šæ ¹æ®learning_rate(ğœ‚)å’Œloss(ğ¿)è®¡ç®—æ¨¡å‹å‚æ•°ã€‚æ¨¡å‹çš„åˆå§‹å‚æ•°w0æ˜¯éšæœºç»™å®šçš„ã€‚
# å³ï¼Œw = optimizer (ğ¿,ğœ‚,w0)
# ä¼˜åŒ–å™¨optimizeråœ¨å·¥ä½œè¿‡ç¨‹ä¸­ï¼Œå®ƒè¿˜ä¼šéšç€è¿­ä»£epochè€Œè°ƒæ•´å­¦ä¹ ç‡learning_rate(ğœ‚)ã€‚æ€è·¯æ˜¯ï¼š
# åˆå§‹é˜¶æ®µå§‹é€‰æ‹©å¤§çš„ğœ‚=ğœ‚0(å³åˆå§‹ğœ‚)ï¼Œéšç€è¿­ä»£çš„æ·±å…¥ï¼Œç”±äºæˆ‘ä»¬åœ¨æ¥è¿‘å±€éƒ¨æˆ–è€…å…¨å±€æœ€ä¼˜loss(ğ¿)ï¼Œæˆ‘ä»¬è°ƒå°ğœ‚ã€‚
# å› æ­¤ï¼Œé€‰æ‹©ä»€ä¹ˆæ ·çš„ä¼˜åŒ–å™¨å†³å®šäº†ğœ‚è°ƒæ•´çš„å¥½åã€‚æœ‰çš„ä¼˜åŒ–å™¨æ–¹æ³•è¿˜æ¶‰åŠåˆ°åŠ¨é‡Momentumçš„è®¾å®šã€‚
# ä¼˜åŒ–å™¨å‡½æ•°optimizerå’ŒæŸå¤±å‡½æ•°lossä¸€èµ·å½±å“æ¨¡å‹æ€§èƒ½ï¼Œlosså‡½æ•°æ˜¯ä¼˜åŒ–å™¨å‡½æ•°optimizerçš„ä¸Šæ¸¸ã€‚
optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)

#-----------------------------è¿­ä»£è®­ç»ƒæ¨¡å‹------------------------------
#ç„¶åè¿­ä»£è¿›è¡Œä»¥ä¸‹æ­¥éª¤ï¼š

#ä» DataLoader ä¸­éšæœºå–ä¸€æ‰¹è®­ç»ƒæ•°æ®ï¼›

#å°†è¿™æ‰¹æ•°æ®é€å…¥æ¨¡å‹ï¼Œè®¡ç®—å‡ºæ¨¡å‹çš„é¢„æµ‹å€¼ï¼›

#å°†æ¨¡å‹é¢„æµ‹å€¼ä¸çœŸå®å€¼è¿›è¡Œæ¯”è¾ƒï¼Œè®¡ç®—æŸå¤±å‡½æ•°ï¼ˆlossï¼‰ã€‚è¿™é‡Œä½¿ç”¨ tf.keras.losses ä¸­çš„äº¤å‰ç†µå‡½æ•°ä½œä¸ºæŸå¤±å‡½æ•°ï¼›

#è®¡ç®—æŸå¤±å‡½æ•°å…³äºæ¨¡å‹å˜é‡çš„å¯¼æ•°ï¼›

#å°†æ±‚å‡ºçš„å¯¼æ•°å€¼ä¼ å…¥ä¼˜åŒ–å™¨ï¼Œä½¿ç”¨ä¼˜åŒ–å™¨çš„ apply_gradients æ–¹æ³•æ›´æ–°æ¨¡å‹å‚æ•°ä»¥æœ€å°åŒ–æŸå¤±å‡½æ•°

for images, labels in dataset:
    # 1.å¼€å§‹æ‰§è¡Œæ¢¯åº¦ä¸‹é™ï¼ˆGradient Descentï¼‰ï¼Œè¿‡ç¨‹ä¸­è®¡ç®—loss
    with tf.GradientTape() as tape:
        # 2.éšæœºèµ‹å€¼æ¨¡å‹çš„å‚æ•°wï¼Œè®¡ç®—loss
        labels_pred = model(images)
        # 2.1 é‡‡ç”¨äº¤å‰ç†µcrossentropyæ–¹æ³•è®¡ç®—lossã€‚åˆé€‚çš„lossè®¡ç®—æ–¹å¼å†³å®šæ¨¡å‹æ€§èƒ½ã€‚
        loss = tf.keras.losses.sparse_categorical_crossentropy(y_true=labels, y_pred=labels_pred)
        loss = tf.reduce_mean(loss)
        print("loss %f" % loss.numpy())
    # 3.æ ¹æ®æ‰€å¾—loss(ğ¿)ï¼Œè®¡ç®—å¯¼æ•°grads=ğœ•ğ¿/ğœ•ğ‘¤
    grads = tape.gradient(loss, model.trainable_variables)
    # 4.ä¾æ®#3æ‰€å¾—å¯¼æ•°ï¼Œå’Œå­¦ä¹ ç‡learning_rate(ğœ‚)ï¼Œå¾—å‡ºä¸‹ä¸€æ­¥çš„æ¨¡å‹çš„å‚æ•°ï¼šğ‘¤ â† ğ‘¤ âˆ’ ğœ‚ğœ•ğ¿/ğœ•ğ‘¤
    optimizer.apply_gradients(grads_and_vars=zip(grads, model.trainable_variables))
